{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def TrainSet():\n",
    "    postList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    label = [0,1,0,1,0,1] #1 is abusive, 0 not\n",
    "    return postList, label\n",
    "def AllWords(postList):\n",
    "    All_word = []\n",
    "    for word_list in postList:\n",
    "        All_word += word_list\n",
    "    return set(All_word)\n",
    "def CountNumWords(allWords, inputSet):\n",
    "    count = np.zeros(len(allWords))\n",
    "    for ips in inputSet:\n",
    "        if ips in allWords:\n",
    "            count[list(allWords).index(ips)] += 1\n",
    "        else:\n",
    "            print(\"{} is beyond our dataset\".format(ips))\n",
    "    return count\n",
    "def NavieBayes(trainMat, label):\n",
    "    num_all_words = len(trainMat[0])\n",
    "    num_document = len(trainMat)\n",
    "    p_priori_1 = np.log((sum(label) + 1) / (num_document + 2))\n",
    "    p_priori_0 = np.log((num_document - sum(label) + 1) / (num_document + 2))\n",
    "    p1 = np.ones(num_all_words)\n",
    "    p0 = np.ones(num_all_words)\n",
    "    p1_denominator = 2\n",
    "    p0_denominator = 2\n",
    "    for i in range(num_document):\n",
    "        if label[i] == 1:\n",
    "            p1 += trainMat[i]\n",
    "            p1_denominator += sum(trainMat[i])\n",
    "        else:\n",
    "            p0 += trainMat[i]\n",
    "            p0_denominator += sum(trainMat[i])\n",
    "    p1 = np.log(p1 / p1_denominator)\n",
    "    p0 = np.log(p0 / p0_denominator)\n",
    "    return p1, p0, p_priori_1, p_priori_0\n",
    "def DetermineCategory(countNumWords, p1, p0, pp1, pp0):\n",
    "    if sum(countNumWords * p1) + pp1 > sum(countNumWords * p0) + pp0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def ClassifierNaiveBayes():\n",
    "    postList, lable = TrainSet()\n",
    "    AW = AllWords(postList)\n",
    "    trainMat = []\n",
    "    for pl in postList:\n",
    "        trainMat.append(CountNumWords(AW, pl))\n",
    "    p1, p0, pp1, pp0 = NavieBayes(trainMat, lable)\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = CountNumWords(AW, testEntry)\n",
    "    print('This document is {}'.format(DetermineCategory(thisDoc, p1, p0, pp1, pp0)))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = CountNumWords(AW, testEntry)\n",
    "    print('This document is {}'.format(DetermineCategory(thisDoc, p1, p0, pp1, pp0)))\n",
    "def TextParse(WholePage):\n",
    "    WordsList = re.split('\\W+', WholePage)\n",
    "    return [word.lower() for word in WordsList if len(word)>=2]\n",
    "def ListOfRandoms(num, rangeMin, rangeMax):\n",
    "    ListRandom = []\n",
    "    while(len(ListRandom) != num):\n",
    "        r = int(np.random.uniform(rangeMin, rangeMax))\n",
    "        if r not in ListRandom:\n",
    "            ListRandom.append(r)\n",
    "    return np.sort(ListRandom)\n",
    "def SpamEmailDetect():\n",
    "    docList = []\n",
    "    label = []\n",
    "    for i in range(1, 26):\n",
    "        docList.append(TextParse(open('email/spam/{}.txt'.format(i)).read()))\n",
    "        label.append(1)\n",
    "        docList.append(TextParse(open('email/ham/{}.txt'.format(i)).read()))\n",
    "        label.append(0)\n",
    "    AW = AllWords(docList)\n",
    "    testSet = []\n",
    "    testLabel = []\n",
    "    indexList = ListOfRandoms(10, 0, 50)\n",
    "    for index in indexList[::-1]:\n",
    "        testSet.append(docList[index])\n",
    "        testLabel.append(label[index])\n",
    "        del docList[index]\n",
    "        del label[index]\n",
    "    trainMat = []\n",
    "    for doc in docList:\n",
    "        trainMat.append(CountNumWords(AW, doc))\n",
    "    p1, p0, pp1, pp0 = NavieBayes(trainMat, label)\n",
    "    error = 0\n",
    "    for i in range(len(testSet)):\n",
    "        thisDoc = CountNumWords(AW, testSet[i])\n",
    "        if DetermineCategory(thisDoc, p1, p0, pp1, pp0) != testLabel[i]:\n",
    "            error = error + 1\n",
    "    #print('Error rate is {}'.format(error/len(testSet)))\n",
    "    return (error/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 0\n",
      "This document is 1\n"
     ]
    }
   ],
   "source": [
    "ClassifierNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error rate is 0.055999999999999994\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "for i in range(100):\n",
    "    error.append(SpamEmailDetect())\n",
    "print('Mean error rate is {}'.format(np.mean(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ny = feedparser.parse('https://newyork.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html': {'class': 'no-js'},\n",
       " 'links': [{'href': 'https://www.craigslist.org/styles/simple-page.css?v=1522ad26a5713653eb46d2c31b48f4fd',\n",
       "   'media': 'all',\n",
       "   'rel': 'stylesheet',\n",
       "   'type': 'text/css'},\n",
       "  {'href': 'https://www.craigslist.org/styles/jquery-ui-clcustom.css?v=3b05ddffb7c7f5b62066deff2dda9339',\n",
       "   'media': 'all',\n",
       "   'rel': 'stylesheet',\n",
       "   'type': 'text/css'},\n",
       "  {'href': 'https://www.craigslist.org/styles/jquery.qtip-2.2.1.css?v=cd202aead4d1dd4894fbae4ade23fcf8',\n",
       "   'media': 'all',\n",
       "   'rel': 'stylesheet',\n",
       "   'type': 'text/css'}],\n",
       " 'meta': {'content': 'width=device-width,initial-scale=1', 'name': 'viewport'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['feed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sortedSF:\n",
    "        print (item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
